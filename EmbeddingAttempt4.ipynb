{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt' \n",
    "# (glove data set from: https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('GloVe Loaded.')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "# Pre-trained GloVe embedding\n",
    "vocab,embd = loadGloVe(filename)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "word_vec_dim = len(embd[0]) # word_vec_dim = dimension of each word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk as nlp\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "summaries = []\n",
    "texts = []\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    printable = set(string.printable)\n",
    "    return filter(lambda x: x in printable, text) #filter funny characters, if any. \n",
    "    \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def split():\n",
    "    titles = []\n",
    "    texts = []\n",
    "    root = 'Part1'\n",
    "    \n",
    "    #dirr = 'Part1/awards_1990/awd_1990_00/'\n",
    "    dirs = os.listdir('Part1/awards_1990/awd_1990_00/')\n",
    "\n",
    "    for filename in dirs[1:]:\n",
    "    #iter = 0\n",
    "    #print(dirs[1])\n",
    "            \n",
    "                #print(iter)\n",
    "                #iter += 1\n",
    "                #print(dirs[1:])\n",
    "                #filename = 'Part1/awards_1990/awd_1990_00/a9000006.txt'\n",
    "        f = open('Part1/awards_1990/awd_1990_00/' + str(filename))\n",
    "        addTitle = False\n",
    "        addTexts = False\n",
    "        title = []\n",
    "        text = []\n",
    "        for word in f.read().split():\n",
    "            if (word == \"Title\"):\n",
    "                addTitle = True\n",
    "                continue\n",
    "\n",
    "            if (word == \"Type\"):\n",
    "                addTitle = False\n",
    "\n",
    "#             if (addTexts == True and word == \"\\n\"):\n",
    "#                 addTexts = False\n",
    "#                 break\n",
    "\n",
    "\n",
    "            if (word == \"Abstract\"):\n",
    "                addTexts = True\n",
    "                continue\n",
    "\n",
    "            if(addTitle == True):\n",
    "                title.append(word)\n",
    "\n",
    "            if(addTexts == True):\n",
    "                text.append(word)\n",
    "\n",
    "        for i in range(len(title)):\n",
    "            s = title[i]\n",
    "            table = str.maketrans({key: None for key in string.punctuation})\n",
    "            new_s = s.translate(table)\n",
    "            title[i] = new_s\n",
    "        for i in range(len(text)):\n",
    "            s = text[i]\n",
    "            table = str.maketrans({key: None for key in string.punctuation})\n",
    "            new_s = s.translate(table)\n",
    "            text[i] = new_s\n",
    "\n",
    "        title = ' '.join(title)\n",
    "        text =' '.join(text)\n",
    "        titles.append(word_tokenize(title))\n",
    "        texts.append(word_tokenize(text))\n",
    "\n",
    "    return titles, texts\n",
    "\n",
    "summaries, texts = split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE CLEANED & TOKENIZED TEXT: \n",
      "\n",
      "['Research', 'in', 'Particulate', 'systems', 'Engineering', 'in', 'crucial', 'to', 'advances', 'in', 'combustion', 'atmospheric', 'and', 'environmental', 'sciences', 'nuclear', 'winter', 'studies', 'nuclear', 'reactor', 'safety', 'and', 'materials', 'manufacturing', 'The', 'purpose', 'of', 'this', 'project', 'is', 'to', 'provide', 'research', 'experiences', 'for', 'undergraduate', 'students', 'in', 'this', 'area', 'At', 'least', 'half', 'of', 'the', 'students', 'will', 'be', 'selected', 'from', 'institutions', 'other', 'the', 'UMC', 'with', 'several', 'recruited', 'from', 'Stephens', 'College', 'a', 'womens', 'college', 'Lincoln', 'University', 'HCBU', 'and', 'other', 'undergraduate', 'schools', 'in', 'the', 'State', 'Each', 'student', 'will', 'undertake', 'a', 'specific', 'research', 'project', 'and', 'a', 'faculty', 'advisor', 'participating', 'in', 'the', 'REU', 'program', 'will', 'work', 'with', 'the', 'student', 'to', 'develop', 'hisher', 'research', 'skills', 'An', 'essential', 'element', 'of', 'the', 'program', 'will', 'be', 'its', 'emphasis', 'on', 'professional', 'development', 'of', 'the', 'students', 'eg', 'paper', 'writing', 'and', 'technical', 'presentations', 'The', 'purpose', 'is', 'to', 'infuse', 'bright', 'undergraduate', 'students', 'with', 'an', 'enthusiasm', 'towards', 'research', 'careers', 'including', 'graduate', 'level', 'education', 'The', 'University', 'of', 'MissouriColumbia', 'has', 'made', 'substantial', 'institutional', 'commitments', 'to', 'the', 'program', 'These', 'include', 'support', 'of', 'faculty', 'time', 'waiver', 'of', 'tuition', 'and', 'fees', 'for', 'the', 'student', 'Participants', 'waiver', 'of', 'all', 'overhead', 'costs', 'and', 'stipends', 'for', 'two', 'minority', 'students', 'in', 'addition', 'to', 'those', 'supported', 'by', 'the', 'NSF', 'funds']\n",
      "\n",
      "SAMPLE CLEANED & TOKENIZED SUMMARY: \n",
      "\n",
      "['Research', 'Expereinces', 'for', 'Undergraduate', 'in', 'Particulate', 'Systems', 'Engineering']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0,len(texts)-1)\n",
    "\n",
    "print (\"SAMPLE CLEANED & TOKENIZED TEXT: \\n\\n\"+str(texts[index]))\n",
    "print (\"\\nSAMPLE CLEANED & TOKENIZED SUMMARY: \\n\\n\"+str(summaries[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_nearest_neighbour(x):\n",
    "    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n",
    "        \n",
    "    xdoty = np.multiply(embedding,x)\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embedding)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "\n",
    "    return embedding[np.argmax(cosine_similarities)]\n",
    "    \n",
    "\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('unk')]\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    for x in xrange(0, len(embedding)):\n",
    "            if np.array_equal(embedding[x],np.asarray(vec)):\n",
    "                return vocab[x]\n",
    "    return vec2word(np_nearest_neighbour(np.asarray(vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'unk':\n",
      "\n",
      "[ -7.91490018e-01   8.66169989e-01   1.19980000e-01   9.22870007e-04\n",
      "   2.77599990e-01  -4.91849989e-01   5.01950026e-01   6.07919996e-04\n",
      "  -2.58450001e-01   1.78650007e-01   2.53500015e-01   7.65720010e-01\n",
      "   5.06640017e-01   4.02500004e-01  -2.13879999e-03  -2.83969998e-01\n",
      "  -5.03239989e-01   3.04490000e-01   5.17790020e-01   1.50899999e-02\n",
      "  -3.50309998e-01  -1.12779999e+00   3.32529992e-01  -3.52499992e-01\n",
      "   4.13260013e-02   1.08630002e+00   3.39099988e-02   3.35640013e-01\n",
      "   4.97449994e-01  -7.01309964e-02  -1.21920002e+00  -4.85119998e-01\n",
      "  -3.85119990e-02  -1.35539994e-01  -1.63800001e-01   5.23209989e-01\n",
      "  -3.13180000e-01  -1.65500000e-01   1.19089998e-01  -1.51150003e-01\n",
      "  -1.56210005e-01  -6.26550019e-01  -6.23359978e-01  -4.21499997e-01\n",
      "   4.18729991e-01  -9.24719989e-01   1.10490000e+00  -2.99959987e-01\n",
      "  -6.30029989e-03   3.95399988e-01]\n"
     ]
    }
   ],
   "source": [
    "word = \"unk\"\n",
    "print (\"Vector representation of '\"+str(word)+\"':\\n\")\n",
    "print (word2vec(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REDUCE DATA (FOR SPEEDING UP THE NEXT STEPS)\n",
    "\n",
    "MAXIMUM_DATA_NUM = 50000\n",
    "\n",
    "texts = texts[0:MAXIMUM_DATA_NUM]\n",
    "summaries = summaries[0:MAXIMUM_DATA_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_limit = []\n",
    "embd_limit = []\n",
    "\n",
    "i=0\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)\n",
    "                embd_limit.append(word2vec(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for summary in summaries:\n",
    "    for word in summary:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)\n",
    "                embd_limit.append(word2vec(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'eos' not in vocab_limit:\n",
    "    vocab_limit.append('eos')\n",
    "    embd_limit.append(word2vec('eos'))\n",
    "if 'unk' not in vocab_limit:\n",
    "    vocab_limit.append('unk')\n",
    "    embd_limit.append(word2vec('unk'))\n",
    "\n",
    "null_vector = np.zeros([word_vec_dim])\n",
    "\n",
    "vocab_limit.append('<PAD>')\n",
    "embd_limit.append(null_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_summaries = []\n",
    "\n",
    "for summary in summaries:\n",
    "    \n",
    "    vec_summary = []\n",
    "    \n",
    "    for word in summary:\n",
    "        vec_summary.append(word2vec(word))\n",
    "            \n",
    "    vec_summary.append(word2vec('eos'))\n",
    "    \n",
    "    vec_summary = np.asarray(vec_summary)\n",
    "    vec_summary = vec_summary.astype(np.float32)\n",
    "    \n",
    "    vec_summaries.append(vec_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_texts = []\n",
    "\n",
    "for text in texts:\n",
    "    \n",
    "    vec_text = []\n",
    "    \n",
    "    for word in text:\n",
    "        vec_text.append(word2vec(word))\n",
    "    \n",
    "    vec_text = np.asarray(vec_text)\n",
    "    vec_text = vec_text.astype(np.float32)\n",
    "    \n",
    "    vec_texts.append(vec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vocab_limit', 'wb') as fp:\n",
    "    pickle.dump(vocab_limit, fp)\n",
    "with open('embd_limit', 'wb') as fp:\n",
    "    pickle.dump(embd_limit, fp)\n",
    "with open('vec_summaries', 'wb') as fp:\n",
    "    pickle.dump(vec_summaries, fp)\n",
    "with open('vec_texts', 'wb') as fp:\n",
    "    pickle.dump(vec_texts, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
