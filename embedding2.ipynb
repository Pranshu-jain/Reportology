{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (config.py, line 733)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/michellezhao/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2862\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-e9cb4ada674a>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import config\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/michellezhao/anaconda3/lib/python3.6/site-packages/config.py\"\u001b[0;36m, line \u001b[0;32m733\u001b[0m\n\u001b[0;31m    except Exception, e:\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate intial word embedding for headlines and description.\n",
    "\n",
    "The embedding is limited to a fixed vocabulary size (`vocab_size`) but\n",
    "a vocabulary of all the words that appeared in the data is built.\n",
    "\"\"\"\n",
    "from os import path\n",
    "import config\n",
    "import _pickle as pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from prep_data import plt\n",
    "\n",
    "# static vars\n",
    "FN = 'vocabulary-embedding'\n",
    "seed = 42\n",
    "vocab_size = 40000\n",
    "embedding_dim = 100\n",
    "lower = False\n",
    "\n",
    "# index words\n",
    "empty = 0  # RNN mask of no data\n",
    "eos = 1  # end of sentence\n",
    "start_idx = eos + 1  # first real word\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def build_vocab(lst):\n",
    "    \"\"\"Return vocabulary for iterable `lst`.\"\"\"\n",
    "    vocab_count = Counter(w for txt in lst for w in txt.split())\n",
    "    vocab = list(map(lambda x: x[0], sorted(vocab_count.items(), key=lambda x: -x[1])))\n",
    "    return vocab, vocab_count\n",
    "\n",
    "\n",
    "def load_text():\n",
    "    \"\"\"Return vocabulary for pickled headlines and descriptions.\"\"\"\n",
    "    # read tokenized headlines and descriptions\n",
    "    with open(path.join(config.path_data, 'tokens.pkl'), 'rb') as fp:\n",
    "        headlines, desc = pickle.load(fp)\n",
    "\n",
    "    # map headlines and descriptions to lower case\n",
    "    if lower:\n",
    "        headlines = [h.lower() for h in headlines]\n",
    "        desc = [h.lower() for h in desc]\n",
    "\n",
    "    return headlines, desc\n",
    "\n",
    "\n",
    "def print_most_popular_tokens(vocab):\n",
    "    \"\"\"Print th most popular tokens in vocabulary dictionary `vocab`.\"\"\"\n",
    "    print('Most popular tokens:')\n",
    "    print(vocab[:50])\n",
    "    print('Total vocab size: {:,}'.format(len(vocab)))\n",
    "\n",
    "\n",
    "def plot_word_distributions(vocab, vocab_count):\n",
    "    \"\"\"Plot word distribution in headlines and discription.\"\"\"\n",
    "    plt.plot([vocab_count[w] for w in vocab])\n",
    "    plt.gca().set_xscale(\"log\", nonposx='clip')\n",
    "    plt.gca().set_yscale(\"log\", nonposy='clip')\n",
    "    title = 'word distribution in headlines and discription'\n",
    "    plt.title(title)\n",
    "    plt.xlabel('rank')\n",
    "    plt.ylabel('total appearances')\n",
    "    plt.savefig(path.join(config.path_outputs, '{}.png'.format(title)))\n",
    "\n",
    "\n",
    "def get_idx(vocab):\n",
    "    \"\"\"Add empty and end-of-sentence tokens to vocabulary and return tuple (vocabulary, reverse-vocabulary).\"\"\"\n",
    "    word2idx = dict((word, idx + start_idx) for idx, word in enumerate(vocab))\n",
    "    word2idx['<empty>'] = empty\n",
    "    word2idx['<eos>'] = eos\n",
    "    idx2word = dict((idx, word) for word, idx in word2idx.items())\n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "def get_glove():\n",
    "    \"\"\"Load GloVe embedding weights and indices.\"\"\"\n",
    "    glove_name = path.join(config.path_data, 'glove.6B.{}d.txt'.format(embedding_dim))\n",
    "    glove_n_symbols = sum(1 for line in open(glove_name))\n",
    "    print('{:,} GloVe symbols'.format(glove_n_symbols))\n",
    "\n",
    "    # load embedding weights and index dictionary\n",
    "    glove_index_dict = {}\n",
    "    glove_embedding_weights = np.empty((glove_n_symbols, embedding_dim))\n",
    "    globale_scale = .1\n",
    "    with open(glove_name, 'r') as fp:\n",
    "        i = 0\n",
    "        for l in fp:\n",
    "            l = l.strip().split()\n",
    "            w = l[0]\n",
    "            glove_index_dict[w] = i\n",
    "            glove_embedding_weights[i, :] = list(map(float, l[1:]))\n",
    "            i += 1\n",
    "    glove_embedding_weights *= globale_scale\n",
    "    print('GloVe std dev: {:.4f}'.format(glove_embedding_weights.std()))\n",
    "\n",
    "    # add lower case version of the keys to the dict\n",
    "    for w, i in glove_index_dict.items():\n",
    "        w = w.lower()\n",
    "        if w not in glove_index_dict:\n",
    "            glove_index_dict[w] = i\n",
    "\n",
    "    return glove_embedding_weights, glove_index_dict\n",
    "\n",
    "\n",
    "def initialize_embedding(vocab_size, embedding_dim, glove_embedding_weights):\n",
    "    \"\"\"Use GloVe to initialize random embedding matrix with same scale as glove.\"\"\"\n",
    "    shape = (vocab_size, embedding_dim)\n",
    "    scale = glove_embedding_weights.std() * np.sqrt(12) / 2  # uniform and not normal\n",
    "    embedding = np.random.uniform(low=-scale, high=scale, size=shape)\n",
    "    print('random-embedding/glove scale: {:.4f} std: {:.4f}'.format(scale, embedding.std()))\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def copy_glove_weights(embedding, idx2word, glove_embedding_weights, glove_index_dict):\n",
    "    \"\"\"Copy from glove weights of words that appear in our short vocabulary (idx2word).\"\"\"\n",
    "    c = 0\n",
    "    for i in range(vocab_size):\n",
    "        w = idx2word[i]\n",
    "        g = glove_index_dict.get(w, glove_index_dict.get(w.lower()))\n",
    "        if g is None and w.startswith('#'):  # glove has no hastags (I think...)\n",
    "            w = w[1:]\n",
    "            g = glove_index_dict.get(w, glove_index_dict.get(w.lower()))\n",
    "        if g is not None:\n",
    "            embedding[i, :] = glove_embedding_weights[g, :]\n",
    "            c += 1\n",
    "    print('number of tokens, in small vocab: {:,} found in glove and copied to embedding: {:.4f}'.format(c, c / float(vocab_size)))\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def build_word_to_glove(embedding, word2idx, idx2word, glove_index_dict, glove_embedding_weights):\n",
    "    \"\"\"Map full vocabulary to glove based on cosine distance.\"\"\"\n",
    "    glove_thr = 0.5\n",
    "    word2glove = {}\n",
    "    for w in word2idx:\n",
    "        if w in glove_index_dict:\n",
    "            g = w\n",
    "        elif w.lower() in glove_index_dict:\n",
    "            g = w.lower()\n",
    "        elif w.startswith('#') and w[1:] in glove_index_dict:\n",
    "            g = w[1:]\n",
    "        elif w.startswith('#') and w[1:].lower() in glove_index_dict:\n",
    "            g = w[1:].lower()\n",
    "        else:\n",
    "            continue\n",
    "        word2glove[w] = g\n",
    "\n",
    "    # for every word outside the embedding matrix find the closest word inside the mebedding matrix.\n",
    "    # Use cos distance of GloVe vectors.\n",
    "    # Allow for the last `nb_unknown_words` words inside the embedding matrix to be considered to be outside.\n",
    "    # Dont accept distances below `glove_thr`\n",
    "    normed_embedding = embedding / np.array(\n",
    "        [np.sqrt(np.dot(gweight, gweight)) for gweight in embedding])[:, None]\n",
    "\n",
    "    nb_unknown_words = 100\n",
    "\n",
    "    glove_match = []\n",
    "    for w, idx in word2idx.items():\n",
    "        if idx >= vocab_size - nb_unknown_words and w.isalpha() and w in word2glove:\n",
    "            gidx = glove_index_dict[word2glove[w]]\n",
    "            gweight = glove_embedding_weights[gidx, :].copy()\n",
    "\n",
    "            # find row in embedding that has the highest cos score with gweight\n",
    "            gweight /= np.sqrt(np.dot(gweight, gweight))\n",
    "            score = np.dot(normed_embedding[:vocab_size - nb_unknown_words], gweight)\n",
    "            while True:\n",
    "                embedding_idx = score.argmax()\n",
    "                s = score[embedding_idx]\n",
    "                if s < glove_thr:\n",
    "                    break\n",
    "                if idx2word[embedding_idx] in word2glove:\n",
    "                    glove_match.append((w, embedding_idx, s))\n",
    "                    break\n",
    "                score[embedding_idx] = -1\n",
    "\n",
    "    glove_match.sort(key=lambda x: -x[2])\n",
    "    print()\n",
    "    print('# of GloVe substitutes found: {:,}'.format(len(glove_match)))\n",
    "\n",
    "    # manually check that the worst substitutions we are going to do are good enough\n",
    "    for orig, sub, score in glove_match[-10:]:\n",
    "        print('{:.4f}'.format(score), orig, '=>', idx2word[sub])\n",
    "\n",
    "    # return a lookup table of index of outside words to index of inside words\n",
    "    return dict((word2idx[w], embedding_idx) for w, embedding_idx, _ in glove_match)\n",
    "\n",
    "\n",
    "def to_dense_vector(word2idx, corpus, description, bins=50):\n",
    "    \"\"\"Create a dense vector representation of headlines.\"\"\"\n",
    "    data = [[word2idx[token] for token in txt.split()] for txt in corpus]\n",
    "    plt.hist(list(map(len, data)), bins=bins)\n",
    "    plt.savefig(path.join(config.path_outputs, '{}_distribution.png'.format(description)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def summarize_vocab(vocab, vocab_count):\n",
    "    \"\"\"Print the most popular tokens and plot token distributions.\"\"\"\n",
    "    print_most_popular_tokens(vocab)\n",
    "    plot_word_distributions(vocab, vocab_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b11dea9f4beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-b11dea9f4beb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\"Generate intial word embedding for headlines and description.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mheadlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load headlines and descriptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadlines\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msummarize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_count\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# summarize vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_text' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Generate intial word embedding for headlines and description.\"\"\"\n",
    "    headlines, desc = load_text()  # load headlines and descriptions\n",
    "    vocab, vocab_count = build_vocab(headlines + desc)  # build vocabulary\n",
    "    summarize_vocab(vocab, vocab_count)  # summarize vocabulary\n",
    "    word2idx, idx2word = get_idx(vocab)  # add special tokens and get reverse vocab lookup\n",
    "    glove_embedding_weights, glove_index_dict = get_glove()  # load GloVe data\n",
    "\n",
    "    # initialize embedding\n",
    "    embedding = initialize_embedding(vocab_size, embedding_dim, glove_embedding_weights)\n",
    "    embedding = copy_glove_weights(embedding, idx2word, glove_embedding_weights, glove_index_dict)\n",
    "\n",
    "    # map vocab to GloVe using cosine similarity\n",
    "    glove_idx2idx = build_word_to_glove(embedding, word2idx, idx2word, glove_index_dict, glove_embedding_weights)\n",
    "\n",
    "    # create a dense vector representation of headlines and descriptions\n",
    "    description_vector = to_dense_vector(word2idx, desc, 'description')\n",
    "    headline_vector = to_dense_vector(word2idx, headlines, 'headline')\n",
    "\n",
    "    # write vocabulary to disk\n",
    "    with open(path.join(config.path_data, '{}.pkl'.format(FN)), 'wb') as fp:\n",
    "        pickle.dump((embedding, idx2word, word2idx, glove_idx2idx), fp, 2)\n",
    "\n",
    "    # write data to disk\n",
    "    with open(path.join(config.path_data, '{}.data.pkl'.format(FN)), 'wb') as fp:\n",
    "        pickle.dump((description_vector, headline_vector), fp, 2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
