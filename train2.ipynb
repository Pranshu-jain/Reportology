{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Define constant variables.\"\"\"\n",
    "\n",
    "# define empty and end-of-sentence vocab idx\n",
    "empty = 0\n",
    "eos = 1\n",
    "\n",
    "# input data (X) is made from maxlend description words followed by eos followed by\n",
    "# headline words followed by eos if description is shorter than maxlend it will be\n",
    "# left padded with empty if entire data is longer than maxlen it will be clipped and\n",
    "# if it is shorter it will be right padded with empty. labels (Y) are the headline\n",
    "# words followed by eos and clipped or padded to maxlenh. In other words the input is\n",
    "# made from a maxlend half in which the description is padded from the left and a\n",
    "# maxlenh half in which eos is followed by a headline followed by another eos if there\n",
    "# is enough space. The labels match only the second half and the first label matches\n",
    "# the eos at the start of the second half (following the description in the first half)\n",
    "maxlend = 100\n",
    "maxlenh = 15\n",
    "maxlen = maxlend + maxlenh\n",
    "activation_rnn_size = 40 if maxlend else 0\n",
    "nb_unknown_words = 10\n",
    "\n",
    "# function names\n",
    "FN0 = 'vocabulary-embedding'  # filename of vocab embeddings\n",
    "FN1 = 'train'  # filename of model weights\n",
    "\n",
    "# training variables\n",
    "seed = 42\n",
    "optimizer = 'adam'\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "regularizer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "#from utils import str_shape\n",
    "#from constants import maxlend, maxlenh, maxlen, activation_rnn_size, optimizer, p_W, p_U, p_dense, p_emb, regularizer\n",
    "\n",
    "\n",
    "def inspect_model(model):\n",
    "    \"\"\"Print the structure of Keras `model`.\"\"\"\n",
    "    for i, l in enumerate(model.layers):\n",
    "        print(i, 'cls={} name={}'.format(type(l).__name__, l.name))\n",
    "        weights = l.get_weights()\n",
    "        print_str = ''\n",
    "#         for weight in weights:\n",
    "#             print_str += str_shape(weight) + ' '\n",
    "        print(print_str)\n",
    "        print()\n",
    "\n",
    "\n",
    "class SimpleContext(Lambda):\n",
    "    \"\"\"Class to implement `simple_context` method as a Keras layer.\"\"\"\n",
    "\n",
    "    def __init__(self, fn, rnn_size, **kwargs):\n",
    "        \"\"\"Initialize SimpleContext.\"\"\"\n",
    "        self.rnn_size = rnn_size\n",
    "        super(SimpleContext, self).__init__(fn, **kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        \"\"\"Compute mask of maxlend.\"\"\"\n",
    "        return input_mask[:, maxlend:]\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        \"\"\"Get output shape for a given `input_shape`.\"\"\"\n",
    "        nb_samples = input_shape[0]\n",
    "        n = 2 * (self.rnn_size - activation_rnn_size)\n",
    "        return (nb_samples, maxlenh, n)\n",
    "\n",
    "\n",
    "def create_model(vocab_size, embedding_size, LR, rnn_layers, rnn_size, embedding=None):\n",
    "    \"\"\"Construct and compile LSTM model.\"\"\"\n",
    "    # create a standard stacked LSTM\n",
    "    if embedding is not None:\n",
    "        embedding = [embedding]\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size,\n",
    "                        input_length=maxlen,\n",
    "                        W_regularizer=regularizer, dropout=p_emb, weights=embedding, mask_zero=True,\n",
    "                        name='embedding_1'))\n",
    "    for i in range(rnn_layers):\n",
    "        lstm = LSTM(rnn_size, return_sequences=True,\n",
    "                    W_regularizer=regularizer, U_regularizer=regularizer,\n",
    "                    b_regularizer=regularizer, dropout_W=p_W, dropout_U=p_U,\n",
    "                    name='lstm_{}'.format(i + 1))\n",
    "        model.add(lstm)\n",
    "        model.add(Dropout(p_dense, name='dropout_{}'.format(i + 1)))\n",
    "\n",
    "    def simple_context(X, mask, n=activation_rnn_size):\n",
    "        \"\"\"Reduce the input just to its headline part (second half).\n",
    "        For each word in this part it concatenate the output of the previous layer (RNN)\n",
    "        with a weighted average of the outputs of the description part.\n",
    "        In this only the last `rnn_size - activation_rnn_size` are used from each output.\n",
    "        The first `activation_rnn_size` output is used to computer the weights for the averaging.\n",
    "        \"\"\"\n",
    "        desc, head = X[:, :maxlend, :], X[:, maxlend:, :]\n",
    "        head_activations, head_words = head[:, :, :n], head[:, :, n:]\n",
    "        desc_activations, desc_words = desc[:, :, :n], desc[:, :, n:]\n",
    "\n",
    "        # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
    "        # activation for every head word and every desc word\n",
    "        activation_energies = K.batch_dot(head_activations, desc_activations, axes=(2, 2))\n",
    "        # make sure we dont use description words that are masked out\n",
    "        activation_energies = activation_energies + -1e20 * K.expand_dims(\n",
    "            1. - K.cast(mask[:, :maxlend], 'float32'), 1)\n",
    "\n",
    "        # for every head word compute weights for every desc word\n",
    "        activation_energies = K.reshape(activation_energies, (-1, maxlend))\n",
    "        activation_weights = K.softmax(activation_energies)\n",
    "        activation_weights = K.reshape(activation_weights, (-1, maxlenh, maxlend))\n",
    "\n",
    "        # for every head word compute weighted average of desc words\n",
    "        desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=(2, 1))\n",
    "        return K.concatenate((desc_avg_word, head_words))\n",
    "\n",
    "    if activation_rnn_size:\n",
    "        model.add(SimpleContext(simple_context, rnn_size, name='simplecontext_1'))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(\n",
    "        vocab_size,\n",
    "        W_regularizer=regularizer,\n",
    "        b_regularizer=regularizer,\n",
    "        name='timedistributed_1')))\n",
    "    model.add(Activation('softmax', name='activation_1'))\n",
    "\n",
    "    # opt = Adam(lr=LR)  # keep calm and reduce learning rate\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    K.set_value(model.optimizer.lr, np.float32(LR))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load constants.py\n",
    "\"\"\"Define constant variables.\"\"\"\n",
    "\n",
    "# define empty and end-of-sentence vocab idx\n",
    "empty = 0\n",
    "eos = 1\n",
    "\n",
    "# input data (X) is made from maxlend description words followed by eos followed by\n",
    "# headline words followed by eos if description is shorter than maxlend it will be\n",
    "# left padded with empty if entire data is longer than maxlen it will be clipped and\n",
    "# if it is shorter it will be right padded with empty. labels (Y) are the headline\n",
    "# words followed by eos and clipped or padded to maxlenh. In other words the input is\n",
    "# made from a maxlend half in which the description is padded from the left and a\n",
    "# maxlenh half in which eos is followed by a headline followed by another eos if there\n",
    "# is enough space. The labels match only the second half and the first label matches\n",
    "# the eos at the start of the second half (following the description in the first half)\n",
    "maxlend = 100\n",
    "maxlenh = 15\n",
    "maxlen = maxlend + maxlenh\n",
    "activation_rnn_size = 40 if maxlend else 0\n",
    "nb_unknown_words = 10\n",
    "\n",
    "# function names\n",
    "FN0 = 'vocabulary-embedding'  # filename of vocab embeddings\n",
    "FN1 = 'train'  # filename of model weights\n",
    "\n",
    "# training variables\n",
    "seed = 42\n",
    "optimizer = 'adam'\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "regularizer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-907f12301f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msample_gen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_split_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minspect_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cs141/sample_gen.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlenh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'empty'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import config\n",
    "from sample_gen import gensamples\n",
    "from utils import prt, load_embedding, process_vocab, load_split_data\n",
    "from model import create_model, inspect_model\n",
    "from generate import gen\n",
    "#from constants import FN1, seed, nb_unknown_words\n",
    "\n",
    "# parse arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch-size', type=int, default=32, help='input batch size')\n",
    "parser.add_argument('--epochs', type=int, default=10, help='number of epochs')\n",
    "parser.add_argument('--rnn-size', type=int, default=512, help='size of RNN layers')\n",
    "parser.add_argument('--rnn-layers', type=int, default=3, help='number of RNN layers')\n",
    "parser.add_argument('--nsamples', type=int, default=640, help='number of samples per epoch')\n",
    "parser.add_argument('--nflips', type=int, default=0, help='number of flips')\n",
    "parser.add_argument('--temperature', type=float, default=.8, help='RNN temperature')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='learning rate, default=0.0001')\n",
    "parser.add_argument('--warm-start', action='store_true')\n",
    "args = parser.parse_args()\n",
    "batch_size = args.batch_size\n",
    "\n",
    "# set sample sizes\n",
    "nb_train_samples = np.int(np.floor(args.nsamples / batch_size)) * batch_size  # num training samples\n",
    "nb_val_samples = nb_train_samples  # num validation samples\n",
    "\n",
    "# seed weight initialization\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "embedding, idx2word, word2idx, glove_idx2idx = load_embedding(nb_unknown_words)\n",
    "vocab_size, embedding_size = embedding.shape\n",
    "oov0 = vocab_size - nb_unknown_words\n",
    "idx2word = process_vocab(idx2word, vocab_size, oov0, nb_unknown_words)\n",
    "X_train, X_test, Y_train, Y_test = load_split_data(nb_val_samples, seed)\n",
    "\n",
    "# print a sample recipe to make sure everything looks right\n",
    "print('Random head, description:')\n",
    "i = 811\n",
    "prt('H', Y_train[i], idx2word)\n",
    "prt('D', X_train[i], idx2word)\n",
    "\n",
    "# save model initialization parameters\n",
    "model_params = (dict(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    LR=args.lr,\n",
    "    rnn_layers=args.rnn_layers,\n",
    "    rnn_size=args.rnn_size,\n",
    "))\n",
    "with open(os.path.join(config.path_models, 'model_params.json'), 'w') as f:\n",
    "    json.dump(model_params, f)\n",
    "\n",
    "\n",
    "model = create_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    LR=args.lr,\n",
    "    embedding=embedding,\n",
    "    rnn_layers=args.rnn_layers,\n",
    "    rnn_size=args.rnn_size,\n",
    ")\n",
    "inspect_model(model)\n",
    "\n",
    "# load pre-trained model weights\n",
    "FN1_filename = os.path.join(config.path_models, '{}.hdf5'.format(FN1))\n",
    "if args.warm_start and FN1 and os.path.exists(FN1_filename):\n",
    "    model.load_weights(FN1_filename)\n",
    "    print('Model weights loaded from {}'.format(FN1_filename))\n",
    "\n",
    "# print samples before training\n",
    "gensamples(\n",
    "    skips=2,\n",
    "    k=10,\n",
    "    batch_size=batch_size,\n",
    "    short=False,\n",
    "    temperature=args.temperature,\n",
    "    use_unk=True,\n",
    "    model=model,\n",
    "    data=(X_test, Y_test),\n",
    "    idx2word=idx2word,\n",
    "    oov0=oov0,\n",
    "    glove_idx2idx=glove_idx2idx,\n",
    "    vocab_size=vocab_size,\n",
    "    nb_unknown_words=nb_unknown_words,\n",
    ")\n",
    "\n",
    "# get train and validation generators\n",
    "r = next(gen(X_train, Y_train, batch_size=batch_size, nb_batches=None, nflips=None, model=None, debug=False, oov0=oov0, glove_idx2idx=glove_idx2idx, vocab_size=vocab_size, nb_unknown_words=nb_unknown_words, idx2word=idx2word))\n",
    "traingen = gen(X_train, Y_train, batch_size=batch_size, nb_batches=None, nflips=args.nflips, model=model, debug=False, oov0=oov0, glove_idx2idx=glove_idx2idx, vocab_size=vocab_size, nb_unknown_words=nb_unknown_words, idx2word=idx2word)\n",
    "valgen = gen(X_test, Y_test, batch_size=batch_size, nb_batches=nb_val_samples // batch_size, nflips=None, model=None, debug=False, oov0=oov0, glove_idx2idx=glove_idx2idx, vocab_size=vocab_size, nb_unknown_words=nb_unknown_words, idx2word=idx2word)\n",
    "\n",
    "# define callbacks for training\n",
    "callbacks = [TensorBoard(\n",
    "    log_dir=os.path.join(config.path_logs, str(time.time())),\n",
    "    histogram_freq=2, write_graph=False, write_images=False)]\n",
    "\n",
    "# train model and save weights\n",
    "h = model.fit_generator(\n",
    "    traingen, samples_per_epoch=nb_train_samples,\n",
    "    nb_epoch=args.epochs, validation_data=valgen, nb_val_samples=nb_val_samples,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "model.save_weights(FN1_filename, overwrite=True)\n",
    "\n",
    "# print samples after training\n",
    "gensamples(\n",
    "    skips=2,\n",
    "    k=10,\n",
    "    batch_size=batch_size,\n",
    "    short=False,\n",
    "    temperature=args.temperature,\n",
    "    use_unk=True,\n",
    "    model=model,\n",
    "    data=(X_test, Y_test),\n",
    "    idx2word=idx2word,\n",
    "    oov0=oov0,\n",
    "    glove_idx2idx=glove_idx2idx,\n",
    "    vocab_size=vocab_size,\n",
    "    nb_unknown_words=nb_unknown_words,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
